{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import NullType, StringType\n",
    "from pyspark.sql.functions import col, lit\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook configuration parameters\n",
    "CREATE_TABLES = True # Set to False to skip table creation\n",
    "FIX_NULLS = True # Set to False to not fix nulls (code will not work)\n",
    "\n",
    "# Postgres configuration parameters\n",
    "POSTGRES_HOST = \"localhost\"\n",
    "POSTGRES_PORT = \"5432\"\n",
    "POSTGRES_DATABASE = \"postgres\"\n",
    "POSTGRES_USER = \"postgres\"\n",
    "POSTGRES_PASSWORD = \"vanguard\" # Would store securely if running in production\n",
    "\n",
    "POSTGRES_URL = \"jdbc:postgresql://{}:{}/{}\".format(POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a Postgres column schema to be used in CREATE TABLE statements\n",
    "def build_column_schema(column_names, column_optionals, column_types):\n",
    "    temp = []\n",
    "    for n in range(0, len(column_names)):\n",
    "        col_definition = column_names[n] + \" \" + column_types[n]\n",
    "        if not column_optionals[n]:\n",
    "            col_definition += \" NOT NULL\"\n",
    "        temp.append(col_definition)\n",
    "    temp.append(\"yyyy integer\")\n",
    "    temp.append(\"yyyymmdd integer\")\n",
    "    temp.append(\"timestamp timestamp\")\n",
    "    temp.append(\"kind varchar\")\n",
    "    column_schema = \", \".join(temp)\n",
    "    return column_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Postgres tables\n",
    "def create_table(input_df, table_name):\n",
    "\n",
    "    # After inspecting the data, this will result in only 1 row, so can use distinct and collect\n",
    "    collected = input_df.select(\"columnnames\", \"columnoptionals\", \"columntypes\", \"schema\").distinct().collect()[0]\n",
    "    column_names = collected[\"columnnames\"]\n",
    "    column_optionals = collected[\"columnoptionals\"]\n",
    "    column_types = collected[\"columntypes\"]\n",
    "    schema = collected[\"schema\"]\n",
    "    \n",
    "    column_schema = build_column_schema(column_names, column_optionals, column_types)\n",
    "    \n",
    "    create_statement = \"CREATE TABLE IF NOT EXISTS {}.{} ({})\".format(schema, table_name, column_schema)\n",
    "    \n",
    "    # Connect to Postgres and create table\n",
    "    conn = psycopg2.connect(\n",
    "        host=POSTGRES_HOST,\n",
    "        database=POSTGRES_DATABASE,\n",
    "        user=POSTGRES_USER,\n",
    "        password=POSTGRES_PASSWORD\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(create_statement)\n",
    "    cur.close()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process incoming data and insert into Postgres\n",
    "def process_data(input_df, table_name, fix_nulls=True):\n",
    "    \n",
    "    collected = input_df.select(\"columnnames\", \"columnoptionals\", \"columntypes\", \"schema\").distinct().collect()[0]\n",
    "    column_names = collected[\"columnnames\"]\n",
    "    schema = collected[\"schema\"]\n",
    "    \n",
    "    select_cols = list(map(\n",
    "        lambda f: col(\"mapped\").getItem(f).alias(str(f)),\n",
    "        column_names\n",
    "    ))\n",
    "    select_cols = select_cols + [\"yyyy\", \"yyyymmdd\", \"timestamp\", \"kind\"]\n",
    "    \n",
    "    # Zip the two arrays and then create a map\n",
    "    # From map, we can select all the fields\n",
    "    output_df = (\n",
    "        input_df\n",
    "        .withColumn(\"mapped\", F.map_from_entries(F.arrays_zip(col(\"columnnames\"), col(\"columnvalues\"))))\n",
    "        .select(select_cols)\n",
    "    )\n",
    "    \n",
    "    # Bug fix - Writes with nulls won't work, so cleaning them to allow code to run\n",
    "    # https://stackoverflow.com/questions/64671739/pyspark-nullable-uuid-type-uuid-but-expression-is-of-type-character-varying\n",
    "    if (fix_nulls):\n",
    "        if (table_name == \"forms\"):\n",
    "            output_df = output_df.fillna({\n",
    "                \"deleted_by\": \"00000000-0000-0000-0000-000000000000\",\n",
    "                \"deleted_at\": \"2999-12-31 23:59:59.000\"\n",
    "            })\n",
    "        elif (table_name == \"questions\"):\n",
    "            output_df = output_df.fillna({\n",
    "                \"deleted_by\": \"00000000-0000-0000-0000-000000000000\",\n",
    "                \"deleted_at\": \"2999-12-31 23:59:59.000\",\n",
    "                \"min_selections\": 0,\n",
    "                \"max_selections\": 0,\n",
    "                \"duplicated_from_question_id\": \"00000000-0000-0000-0000-000000000000\",\n",
    "            })\n",
    "        elif (table_name == \"submissions\"):\n",
    "            output_df = output_df.fillna({\n",
    "                \"updated_by\": \"00000000-0000-0000-0000-000000000000\",\n",
    "                \"context_id\": \"00000000-0000-0000-0000-000000000000\",\n",
    "                \"created_by\": \"00000000-0000-0000-0000-000000000000\",\n",
    "            })\n",
    "    \n",
    "    (\n",
    "        output_df\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        #.option(\"createTableColumnTypes\", column_schema) # SEE APPROACH #1 IN NOTES\n",
    "        .jdbc(POSTGRES_URL, schema + \".\" + table_name,\n",
    "             properties = {\n",
    "                 \"user\": POSTGRES_USER,\n",
    "                 \"password\": POSTGRES_PASSWORD,\n",
    "                 \"stringtype\": \"unspecified\"\n",
    "             }\n",
    "             )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN WORKFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "# Added postgresql JDBC driver to the class path\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local\").appName(\"vanguard\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.2.18.jar\")\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.2.18.jar\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the write ahead logs\n",
    "data = spark.read.format(\"json\").option(\"basePath\", \"data\").load(\"data/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the change object so there's only one database transaction per row\n",
    "exploded = (\n",
    "    data\n",
    "    .withColumn(\"change\", F.explode(\"change\"))\n",
    "    .select(\n",
    "        \"change.columnnames\",\n",
    "        \"change.columnoptionals\",\n",
    "        \"change.columntypes\",\n",
    "        \"change.columnvalues\",\n",
    "        \"change.kind\",\n",
    "        \"change.table\",\n",
    "        \"change.schema\",\n",
    "        \"yyyy\",\n",
    "        \"yyyymmdd\",\n",
    "        \"timestamp\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each table and write to Postgres\n",
    "for table_name in [\"forms\", \"questions\", \"responses\", \"submissions\"]:\n",
    "    input_df = exploded.filter(col(\"table\") == table_name)\n",
    "    if CREATE_TABLES:\n",
    "        create_table(input_df, table_name)\n",
    "    process_data(input_df, table_name, FIX_NULLS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
